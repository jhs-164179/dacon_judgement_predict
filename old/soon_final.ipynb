{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, ElasticNet\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Seed\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train = pd.read_csv('./data/train.csv').drop('ID', axis=1)\n",
    "test = pd.read_csv('./data/test.csv').drop('ID', axis=1)\n",
    "submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing(1)\n",
    "cols = ['first_party', 'second_party', 'facts']\n",
    "\n",
    "for col in cols:\n",
    "    # 문자열 데이터 좌우 공백 제거\n",
    "    train[col] = train[col].str.strip()\n",
    "    test[col] = test[col].str.strip()\n",
    "\n",
    "    # 두 칸 이상의 공백 한 칸으로 변경\n",
    "    train[col] = train[col].str.replace('  ', ' ')\n",
    "    test[col] = train[col].str.replace('  ', ' ')\n",
    "\n",
    "    # 소문자로 변경\n",
    "    train[col] = train[col].str.lower()\n",
    "    test[col] = test[col].str.lower()\n",
    "\n",
    "    # \",\", \".\" 제거\n",
    "    train[col] = train[col].str.replace(',','')\n",
    "    train[col] = train[col].str.replace('.','')\n",
    "    test[col] = test[col].str.replace(',','')\n",
    "    test[col] = test[col].str.replace('.','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing(2)\n",
    "first_party_list = []\n",
    "first_party_list_test = []\n",
    "second_party_list = []\n",
    "second_party_list_test = []\n",
    "facts_list = []\n",
    "facts_list_test = []\n",
    "\n",
    "shortword = re.compile(r'\\W*\\b\\w{1}\\b')\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "stopword_list = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# first_party\n",
    "\n",
    "## train\n",
    "for first in train['first_party']:\n",
    "    # 1글자 단어 제거\n",
    "    first = shortword.sub('', first)\n",
    "    # 특수문자 제거\n",
    "    first = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", first)\n",
    "    # TreebankWordTokenizer를 이용한 단어 토큰화\n",
    "    token = tokenizer.tokenize(first)\n",
    "    # 불용어 제거\n",
    "    new_token = []\n",
    "    for tok in token:\n",
    "        if tok not in stopword_list:\n",
    "            # 표제어 추출\n",
    "            new_token.append(lemmatizer.lemmatize(tok, 'n'))\n",
    "\n",
    "    first_party_list.append(new_token)\n",
    "\n",
    "# sklearn.feature_extraction 변환을 위해 단어들을 하나로 결합\n",
    "for i in range(len(first_party_list)):\n",
    "    first_party_list[i] = ' '.join(first_party_list[i])\n",
    "\n",
    "## test\n",
    "for first in test['first_party']:\n",
    "    # 1글자 단어 제거\n",
    "    first = shortword.sub('', first)\n",
    "    # 특수문자 제거\n",
    "    first = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", first)\n",
    "    # TreebankWordTokenizer를 이용한 단어 토큰화\n",
    "    token = tokenizer.tokenize(first)\n",
    "    # 불용어 제거\n",
    "    new_token = []\n",
    "    for tok in token:\n",
    "        if tok not in stopword_list:\n",
    "            # 표제어 추출\n",
    "            new_token.append(lemmatizer.lemmatize(tok, 'n'))\n",
    "\n",
    "    first_party_list_test.append(new_token)\n",
    "\n",
    "# sklearn.feature_extraction 변환을 위해 단어들을 하나로 결합\n",
    "for i in range(len(first_party_list_test)):\n",
    "    first_party_list_test[i] = ' '.join(first_party_list_test[i])\n",
    "\n",
    "\n",
    "# second_party\n",
    "\n",
    "## train\n",
    "for second in train['second_party']:\n",
    "    # 1글자 단어 제거\n",
    "    second = shortword.sub('', second)\n",
    "    # 특수문자 제거\n",
    "    second = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", second)\n",
    "    # TreebankWordTokenizer를 이용한 단어 토큰화\n",
    "    token = tokenizer.tokenize(second)\n",
    "    # 불용어 제거\n",
    "    new_token = []\n",
    "    for tok in token:\n",
    "        if tok not in stopword_list:\n",
    "            # 표제어 추출\n",
    "            new_token.append(lemmatizer.lemmatize(tok, 'n'))\n",
    "\n",
    "    second_party_list.append(new_token)\n",
    "\n",
    "# sklearn.feature_extraction 변환을 위해 단어들을 하나로 결합\n",
    "for i in range(len(second_party_list)):\n",
    "    second_party_list[i] = ' '.join(second_party_list[i])\n",
    "\n",
    "## test\n",
    "for second in test['second_party']:\n",
    "    # 1글자 단어 제거\n",
    "    second = shortword.sub('', second)\n",
    "    # 특수문자 제거\n",
    "    second = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", second)\n",
    "    # TreebankWordTokenizer를 이용한 단어 토큰화\n",
    "    token = tokenizer.tokenize(second)\n",
    "    # 불용어 제거\n",
    "    new_token = []\n",
    "    for tok in token:\n",
    "        if tok not in stopword_list:\n",
    "            # 표제어 추출\n",
    "            new_token.append(lemmatizer.lemmatize(tok, 'n'))\n",
    "\n",
    "    second_party_list_test.append(new_token)\n",
    "\n",
    "# sklearn.feature_extraction 변환을 위해 단어들을 하나로 결합\n",
    "for i in range(len(second_party_list_test)):\n",
    "    second_party_list_test[i] = ' '.join(second_party_list_test[i])\n",
    "\n",
    "\n",
    "# facts\n",
    "\n",
    "## train\n",
    "for fact in train['facts']:\n",
    "    # 1글자 단어 제거\n",
    "    fact = shortword.sub('', fact)\n",
    "    # 특수문자 제거\n",
    "    fact = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", fact)\n",
    "    # TreebankWordTokenizer를 이용한 단어 토큰화\n",
    "    token = tokenizer.tokenize(fact)\n",
    "    # 불용어 제거\n",
    "    new_token = []\n",
    "    for tok in token:\n",
    "        if tok not in stopword_list:\n",
    "            new_token.append(tok)\n",
    "\n",
    "    facts_list.append(new_token)\n",
    "\n",
    "# sklearn.feature_extraction 변환을 위해 단어들을 하나로 결합\n",
    "for i in range(len(facts_list)):\n",
    "    facts_list[i] = ' '.join(facts_list[i])\n",
    "\n",
    "## test\n",
    "for fact in test['facts']:\n",
    "    # 1글자 단어 제거\n",
    "    fact = shortword.sub('', fact)\n",
    "    # 특수문자 제거\n",
    "    fact = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", fact)\n",
    "    # TreebankWordTokenizer를 이용한 단어 토큰화\n",
    "    token = tokenizer.tokenize(fact)\n",
    "    # 불용어 제거\n",
    "    new_token = []\n",
    "    for tok in token:\n",
    "        if tok not in stopword_list:\n",
    "            new_token.append(tok)\n",
    "\n",
    "    facts_list_test.append(new_token)\n",
    "\n",
    "# sklearn.feature_extraction 변환을 위해 단어들을 하나로 결합\n",
    "for i in range(len(facts_list_test)):\n",
    "    facts_list_test[i] = ' '.join(facts_list_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing(3)\n",
    "# first, second_party -> count벡터화\n",
    "# facts -> tfidf벡터화\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "vectorizer_fact = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "vectorizer.fit(first_party_list + second_party_list)\n",
    "vectorizer_fact.fit(facts_list)\n",
    "\n",
    "X1 = vectorizer.transform(first_party_list).toarray()\n",
    "X2 = vectorizer.transform(second_party_list).toarray()\n",
    "X3 = vectorizer.transform(facts_list).toarray()\n",
    "\n",
    "X_train = np.concatenate([X1, X2, X3], axis=1)\n",
    "\n",
    "X1 = vectorizer.transform(first_party_list_test).toarray()\n",
    "X2 = vectorizer.transform(second_party_list_test).toarray()\n",
    "X3 = vectorizer.transform(facts_list_test).toarray()\n",
    "\n",
    "X_test = np.concatenate([X1, X2, X3], axis=1)\n",
    "\n",
    "y_train = train['first_party_winner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<train 데이터>\n",
      "(2478, 33984) (2478,)\n",
      "\n",
      "<test 데이터>\n",
      "(1240, 33984)\n"
     ]
    }
   ],
   "source": [
    "print('<train 데이터>')\n",
    "print(X_train.shape, y_train.shape)\n",
    "print()\n",
    "print('<test 데이터>')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언더샘플링\n",
    "X_NC, Y_NC = NeighbourhoodCleaningRule(n_neighbors=5).fit_resample(X_train, y_train)\n",
    "train_x, val_x, train_y, val_y = train_test_split(X_NC, Y_NC, test_size=.2, stratify=Y_NC, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "first_party_winner\n",
       "1    1649\n",
       "0     829\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "NCRule DownSampling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "first_party_winner\n",
       "1    1007\n",
       "0     829\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "first_party_winner\n",
       "1    805\n",
       "0    663\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Validation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "first_party_winner\n",
       "1    202\n",
       "0    166\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Original')\n",
    "display(y_train.value_counts())\n",
    "print('='*30)\n",
    "print('NCRule DownSampling')\n",
    "display(Y_NC.value_counts())\n",
    "print('='*30)\n",
    "print('Train')\n",
    "display(train_y.value_counts())\n",
    "print('='*30)\n",
    "print('Validation')\n",
    "display(val_y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom sklearn.linear_model import LinearRegression, LogisticRegression, ElasticNet\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.59      0.54       166\n",
      "           1       0.60      0.51      0.56       202\n",
      "\n",
      "    accuracy                           0.55       368\n",
      "   macro avg       0.55      0.55      0.55       368\n",
      "weighted avg       0.56      0.55      0.55       368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Linear = LinearRegression()\n",
    "Linear.fit(train_x, train_y)\n",
    "print(classification_report(val_y, np.where(Linear.predict(val_x)>.5, 1, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.52      0.52       166\n",
      "           1       0.61      0.61      0.61       202\n",
      "\n",
      "    accuracy                           0.57       368\n",
      "   macro avg       0.57      0.57      0.57       368\n",
      "weighted avg       0.57      0.57      0.57       368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Logistic = LogisticRegression(max_iter=500, random_state=42)\n",
    "Logistic.fit(train_x, train_y)\n",
    "print(classification_report(val_y, Logistic.predict(val_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ggyo0\\AppData\\Local\\Temp\\ipykernel_18864\\1328398427.py:3: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  Lasso.fit(train_x, train_y)\n",
      "c:\\jhs\\project\\dacon_crime\\jhs_crime\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.42      0.44       166\n",
      "           1       0.56      0.59      0.57       202\n",
      "\n",
      "    accuracy                           0.52       368\n",
      "   macro avg       0.51      0.51      0.51       368\n",
      "weighted avg       0.51      0.52      0.51       368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "Lasso = ElasticNet(alpha=0, random_state=42)\n",
    "Lasso.fit(train_x, train_y)\n",
    "print(classification_report(val_y, np.where(Lasso.predict(val_x)>.5, 1, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.23      0.33       166\n",
      "           1       0.57      0.83      0.68       202\n",
      "\n",
      "    accuracy                           0.56       368\n",
      "   macro avg       0.55      0.53      0.50       368\n",
      "weighted avg       0.55      0.56      0.52       368\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\jhs\\project\\dacon_crime\\jhs_crime\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.661e+02, tolerance: 3.636e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "Ridge = ElasticNet(l1_ratio=0, random_state=42)\n",
    "Ridge.fit(train_x, train_y)\n",
    "print(classification_report(val_y, np.where(Ridge.predict(val_x)>.5, 1, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       166\n",
      "           1       0.55      1.00      0.71       202\n",
      "\n",
      "    accuracy                           0.55       368\n",
      "   macro avg       0.27      0.50      0.35       368\n",
      "weighted avg       0.30      0.55      0.39       368\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\jhs\\project\\dacon_crime\\jhs_crime\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\jhs\\project\\dacon_crime\\jhs_crime\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\jhs\\project\\dacon_crime\\jhs_crime\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "Elastic = ElasticNet(random_state=42)\n",
    "Elastic.fit(train_x, train_y)\n",
    "print(classification_report(val_y, np.where(Elastic.predict(val_x)>.5, 1, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.20      0.31       166\n",
      "           1       0.59      0.94      0.72       202\n",
      "\n",
      "    accuracy                           0.60       368\n",
      "   macro avg       0.65      0.57      0.52       368\n",
      "weighted avg       0.65      0.60      0.54       368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Tree = DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=42)\n",
    "Tree.fit(train_x, train_y)\n",
    "print(classification_report(val_y, Tree.predict(val_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhs_crime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
