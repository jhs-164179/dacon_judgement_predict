{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Seed\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train = pd.read_csv('./data/train.csv').drop('ID', axis=1)\n",
    "test = pd.read_csv('./data/test.csv').drop('ID', axis=1)\n",
    "submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "- 문자열 전처리, 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열 전처리\n",
    "cols = ['first_party', 'second_party', 'facts']\n",
    "shortword = re.compile(r'\\W*\\b\\w{1}\\b')\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "stopword = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 전처리 함수 1\n",
    "def preprocessing(df, cols, shortword, tokenizer, stopword, lemmatizer):    \n",
    "    first_party_lst = []\n",
    "    second_party_lst = []\n",
    "    facts_lst = []\n",
    "    for col in cols:\n",
    "        # 좌우 공백 제거\n",
    "        df[col] = df[col].str.strip()\n",
    "        # 두 칸 이상의 공백 한 칸으로 변경\n",
    "        df[col] = df[col].str.replace('  ', ' ')\n",
    "        # 소문자로 변경\n",
    "        df[col] = df[col].str.lower()\n",
    "        # \",\", \".\" 제거\n",
    "        df[col] = df[col].str.replace(',','')\n",
    "        df[col] = df[col].str.replace('.','')\n",
    "\n",
    "        if col == 'first_party':\n",
    "            for sample in df[col]:\n",
    "                # 한글자 단어 제거\n",
    "                sample = shortword.sub('', sample)\n",
    "                # 특수문자 제거\n",
    "                sample = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", sample)\n",
    "                # tokenzier를 이용한 단어 토큰화\n",
    "                token = tokenizer.tokenize(sample)\n",
    "                # 불용어 제거\n",
    "                new_token = []\n",
    "                for tok in token:\n",
    "                    if tok not in stopword:\n",
    "                        # 표제어 추출\n",
    "                        new_token.append(lemmatizer.lemmatize(tok, 'n'))\n",
    "                first_party_lst.append(new_token)\n",
    "            # sklearn.feature_extraction 변환을 위해 단어들을 결합\n",
    "            for i in range(len(first_party_lst)):\n",
    "                first_party_lst[i] = ' '.join(first_party_lst[i])\n",
    "\n",
    "        elif col == 'second_party':\n",
    "            for sample in df[col]:\n",
    "                # 한글자 단어 제거\n",
    "                sample = shortword.sub('', sample)\n",
    "                # 특수문자 제거\n",
    "                sample = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", sample)\n",
    "                # tokenzier를 이용한 단어 토큰화\n",
    "                token = tokenizer.tokenize(sample)\n",
    "                # 불용어 제거\n",
    "                new_token = []\n",
    "                for tok in token:\n",
    "                    if tok not in stopword:\n",
    "                        # 표제어 추출\n",
    "                        new_token.append(lemmatizer.lemmatize(tok, 'n'))\n",
    "                second_party_lst.append(new_token)\n",
    "            # sklearn.feature_extraction 변환을 위해 단어들을 결합\n",
    "            for i in range(len(second_party_lst)):\n",
    "                second_party_lst[i] = ' '.join(second_party_lst[i])\n",
    "\n",
    "        elif col=='facts':\n",
    "            for sample in df[col]:\n",
    "                # 한글자 단어 제거\n",
    "                sample = shortword.sub('', sample)\n",
    "                # 특수문자 제거\n",
    "                sample = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", sample)\n",
    "                # tokenzier를 이용한 단어 토큰화\n",
    "                token = tokenizer.tokenize(sample)\n",
    "                # 불용어 제거\n",
    "                new_token = []\n",
    "                for tok in token:\n",
    "                    if tok not in stopword:\n",
    "                        new_token.append(tok)\n",
    "                facts_lst.append(new_token)\n",
    "            # sklearn.feature_extraction 변환을 위해 단어들을 결합\n",
    "            for i in range(len(facts_lst)):\n",
    "                facts_lst[i] = ' '.join(facts_lst[i])\n",
    "\n",
    "        else:\n",
    "            print('컬럼이름을 변경하지 말아주세요!')\n",
    "\n",
    "    return first_party_lst, second_party_lst, facts_lst\n",
    "\n",
    "# 전처리 함수 2(벡터화)                \n",
    "def preprocessing_2(first, second, facts, vec, vec_facts, train=True):\n",
    "    if train:\n",
    "        vec.fit(first + second)\n",
    "        vec_facts.fit(facts)\n",
    "\n",
    "    X1 = vec.transform(first).toarray()\n",
    "    X2 = vec.transform(second).toarray()\n",
    "    X3 = vec_facts.transform(facts).toarray()\n",
    "\n",
    "    return np.concatenate([X1, X2, X3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열 전처리 1\n",
    "cols = ['first_party', 'second_party', 'facts']\n",
    "shortword = re.compile(r'\\W*\\b\\w{1}\\b')\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "stopword = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "first_train, second_train, facts_train = preprocessing(train, cols, shortword, tokenizer, stopword, lemmatizer)\n",
    "first_test, second_test, facts_test = preprocessing(test, cols, shortword, tokenizer, stopword, lemmatizer)\n",
    "\n",
    "# 문자열 전처리 2(벡터화)\n",
    "vec = CountVectorizer(ngram_range=(1,2))\n",
    "vec_facts = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "X_train = preprocessing_2(first_train, second_train, facts_train, vec, vec_facts)\n",
    "y_train = train['first_party_winner']\n",
    "X_test = preprocessing_2(first_test, second_test, facts_test, vec, vec_facts, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<train 데이터>\n",
      "(2478, 211292) (2478,)\n",
      "\n",
      "<test 데이터>\n",
      "(1240, 211292)\n"
     ]
    }
   ],
   "source": [
    "print('<train 데이터>')\n",
    "print(X_train.shape, y_train.shape)\n",
    "print()\n",
    "print('<test 데이터>')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing2\n",
    "- 불균형 데이터 전처리(다운샘플링)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape after UnderSampling\n",
      "(1643, 211292) (1643,)\n",
      "====================\n",
      "Train target after UnderSampling\n",
      "first_party_winner\n",
      "0    829\n",
      "1    814\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불균형 문제 전처리(언더샘플링)\n",
    "X_nc, y_nc = NeighbourhoodCleaningRule(n_neighbors=3).fit_resample(X_train, y_train)\n",
    "print('Train Data Shape after UnderSampling')\n",
    "print(X_nc.shape, y_nc.shape)\n",
    "print('='*20)\n",
    "print('Train target after UnderSampling')\n",
    "print(y_nc.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape\n",
      "(1232, 211292) (1232,)\n",
      "--------------------\n",
      "Train target\n",
      "first_party_winner\n",
      "0    622\n",
      "1    610\n",
      "Name: count, dtype: int64\n",
      "====================\n",
      "Validation Data Shape\n",
      "(411, 211292) (411,)\n",
      "--------------------\n",
      "Validation target\n",
      "first_party_winner\n",
      "0    207\n",
      "1    204\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Train, Validation 분리\n",
    "Train_X, Val_X, Train_y, Val_y = train_test_split(X_nc, y_nc, test_size=.25, random_state=42, stratify=y_nc)\n",
    "print('Train Data Shape')\n",
    "print(Train_X.shape, Train_y.shape)\n",
    "print('-'*20)\n",
    "print('Train target')\n",
    "print(Train_y.value_counts())\n",
    "print('='*20)\n",
    "print('Validation Data Shape')\n",
    "print(Val_X.shape, Val_y.shape)\n",
    "print('-'*20)\n",
    "print('Validation target')\n",
    "print(Val_y.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.68       207\n",
      "           1       0.67      0.64      0.66       204\n",
      "\n",
      "    accuracy                           0.67       411\n",
      "   macro avg       0.67      0.67      0.67       411\n",
      "weighted avg       0.67      0.67      0.67       411\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Logistic = LogisticRegression(max_iter=500, random_state=42)\n",
    "Logistic.fit(Train_X, Train_y)\n",
    "print(classification_report(Val_y, Logistic.predict(Val_X)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['first_party_winner'] = Logistic.predict(X_test)\n",
    "submission.to_csv('logi___2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhs_legal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
